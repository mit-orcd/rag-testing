Engaging Cluster Documentation1. Cluster workflow :: Engaging Cluster DocumentationEngaging Cluster Documentation1. Logging into the cluster2. Replacing or Adding an SSH KeySetup SSH Key Pair Authentication3. Slurm1. Cluster workflow2. Slurm Job Scheduler3. sbatch4. srun & salloc5. Slurm Job Arrays6. Determining Resources For Your Job4. Software1. Loading Software Modules2. Python Packages3. R Packages4. Compiling Software For Personal Use5. Compiling Miniconda For Personal Use6. Running Paraview in Client Server Mode via Slurm5. Storage1. The Home directory2. The Lustre File System3. NFS StorageFile Permissions6. Virtual Environments1. Jupyter Notebooks2. Python Packages3. Python Virtual Environments4. Anaconda Virtual Environments7. Best PracticesLustre Best Practices8. Frequently Asked QuestionsImprove this page>3. Slurm> 1. Cluster workflow1. Cluster workflowOnce you’ve followed the stepshere, you will be logged into what is referred to as a “login node” or “head node”. These are the nodes that connect the cluster to the outside world. They are intended for basic tasks such as:Managing files.Submitting jobs to the compute nodes.Uploading and downloading data.Compiling software.While small scale interactive code and tests are allowed to be run on the login nodes, please remember that the login nodes are shared by all users so any resource intensive code must be handled by the compute nodes. Any resource intensive jobs found running on login nodes are subject to beingkilled without warning.To run a job on the compute nodes you will need to use theslurmjob scheduler to submit your jobs to the compute nodes. This can be done with eithersrunfor interactive runs orsbatchfor batch jobs.To compile software on the cluster for personal use, please seehere.3. Slurm2. Slurm Job Scheduler