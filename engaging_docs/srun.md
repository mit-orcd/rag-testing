Engaging Cluster Documentation4. srun & salloc :: Engaging Cluster DocumentationEngaging Cluster Documentation1. Logging into the clusterLogging into Engaging Open OnDemandLogging into FastXLogging in via sshLogging in from a Linux or OSX hostLogging in from a Windows host2. Replacing or Adding an SSH KeySetup SSH Key Pair Authentication3. Slurm1. Cluster workflow2. Slurm Job Scheduler3. sbatch4. srun & salloc5. Slurm Job Arrays6. Determining Resources For Your Job4. Software1. Loading Software Modules2. Python Packages3. R Packages4. Compiling Software For Personal Use5. Compiling Miniconda For Personal Use6. Running Paraview in Client Server Mode via Slurm5. Storage1. The Home directory2. The Lustre File System3. NFS StorageFile Permissions6. Virtual Environments1. Jupyter Notebooks2. Python Packages3. Python Virtual Environments4. Anaconda Virtual Environments7. Best PracticesLustre Best Practices8. Frequently Asked QuestionsImprove this page>3. Slurm> 4. srun & salloc4. srun & sallocSRUN & SALLOCInteractive jobs can be run withsrunorsalloc.Note that both of these commands take slurm directives as command line arguments rather than #SBATCH directives in a file.Differences Between SBATCH and SRUNCompare the following srun command to the sbatch script found on thesbatch pagesrun -N 1 -n 4 -t 0-00:30 -C centos7 \
 -p sched_engaging_default --mem-per-cpu=4000 \
-o output_%j.txt -e error_%j.txt \
--mail-type=BEGIN,END --mail-user=test@test.com \
echo "Hello, World"Generally we recommend running in batch mode when possible as your job will run even if your terminal session dies, but there are a few use cases for srun.Using srun to get a shell on a compute node:srun -N 1 -n 1 --pty /bin/bashRunning a job with X11 forwarding enabledIf you need to run an interactive job with X11 forwarding to open interactive GUI windows, such as Firefox or MATLAB, you can request a node with a command similar to the one below. This command allocates one node and one core, similar to the above example, but with X forwarding.srun -N 1 -n 1 --x11 --pty bashOnce the allocation has been granted, the command to launch the GUI application can be ran. On some systems, you may be able to launch X commands, such as xeyes or xclock, without having to request a shell on a node. Please note that not all Engaging systems have these X applications.More information on srun is available from SchedMD:https://slurm.schedmd.com/srun.htmlUsing SALLOCsallocis generally used in the same interactive manner assrun.A job can be sumitted interactively, with the output being returned to your terminal:salloc -N 1 -n 1 echo "Hello, World"Using salloc to get a shell on a compute node:salloc -N 1 -n 1Notice how the pseudo-teletype flag “–pty” followed by a shell are not required. If no command is specified at the end, salloc will run the users default shell.Differences between SALLOC and SRUNsalloc(likesbatch) allocate resources to run a job, whilesrunlaunches parallel tasks across those resources.sruncan be used to launch parallel tasks across some or all of the allocated resources.sruncan be ran inside of ansbatchscript to run tasks in parallel, in which it will inherit the pertinent arguments or options. In the above examples,sruncan also be used on its own to request resources and launch tasks, where tasks will be spread across those resources as a single job.More information on salloc is available from SchedMD:https://slurm.schedmd.com/salloc.htmlIf you have any questions about using srun, please emailorcd-help-engaging@mit.edu3. sbatch5. Slurm Job Arrays