Engaging Cluster Documentation2. Slurm Job Scheduler :: Engaging Cluster DocumentationEngaging Cluster Documentation1. Logging into the clusterLogging into Engaging Open OnDemandLogging into FastXLogging in via sshLogging in from a Linux or OSX hostLogging in from a Windows host2. Replacing or Adding an SSH KeySetup SSH Key Pair Authentication3. Slurm1. Cluster workflow2. Slurm Job Scheduler3. sbatch4. srun & salloc5. Slurm Job Arrays6. Determining Resources For Your Job4. Software1. Loading Software Modules2. Python Packages3. R Packages4. Compiling Software For Personal Use5. Compiling Miniconda For Personal Use6. Running Paraview in Client Server Mode via Slurm5. Storage1. The Home directory2. The Lustre File System3. NFS StorageFile Permissions6. Virtual Environments1. Jupyter Notebooks2. Python Packages3. Python Virtual Environments4. Anaconda Virtual Environments7. Best PracticesLustre Best Practices8. Frequently Asked QuestionsImprove this page>3. Slurm> 2. Slurm Job Scheduler2. Slurm Job SchedulerThe Slurm Job SchedulerAll jobs run on the cluster must be submitted through the slurm job scheduler.Slurm’s purpose is to  fairly and efficiently allocate resources amongst the compute nodes available on the cluster. It is imperative that you run your job on the compute nodes by submitting the job to the job scheduler with eithersbatchorsrun.*Any resource intensive jobs found running on login nodes are subject to beingkilled without warning.*Running jobs:There are two ways to submit jobs to the cluster:Submitting a script withsbatchInteractively withsrunChecking on JobsThere are a few commands that will give information on pending, running, and completed jobs.sinfo- view information about Slurm nodes and partitions.scontrol- view Slurm configuration and state.squeue-  view  information about jobs located in the Slurm scheduling queue.sacct-  displays  accounting  data  for all jobs and job steps in the Slurm job accounting log or Slurm database.Useful Slurm CommandsSee which partitions you can submit tosinfoSee all of the nodes in a certain partition, nodes listed as down and/or drain cannot be used to run jobs.sinfo -p <partitionname>See information about a job currently in the queue.scontrol show jobid <jobid>See information about node(s), useful when combined withscontrol show jobscontrol show node <nodelist>Note: You can pass either a single node or a nodelist (node[001-010]) to this.See information about a partition, useful for things like finding out the MaxWallTime or the number of nodes of a partition.scontrol show partition <partitionname>Look up the status of a job by jobIDsqueue -j <jobid>See the status of your jobs in the queuesqueue -u <username>See when Slurm thinks your job will start, and some reasons why it is waitingsqueue -u <username> --startPossible reasons your job is pending:(Resources) - Job is waiting for compute nodes to become available(Priority) - Jobs with a higher priority score are waiting compute nodes.(ReqNodeNotAvail) - The compute nodes requested by the job are not available, might be due to things such as cluster maintenance, nodes are down/offline, scheduler is backed up with too many jobs, or the config you requested just doesn’t exist.Get some information on a specific job that has completed.sacct -j <jobid> --format=JobID,JobName,Elapsed,StateGet some information on a specific job that has completed today, you can use the -S flag to specify a start time to see stats on jobs from further back.sacct -u <username> --format=JobID,JobName,Elapsed,StateFurther documentation on Slurm commands:https://slurm.schedmd.com/sinfo.htmlhttps://slurm.schedmd.com/scontrol.htmlhttps://slurm.schedmd.com/sacct.htmlhttps://slurm.schedmd.com/squeue.htmlIf you have any questions about using slurm, please emailorcd-help-engaging@mit.edu1. Cluster workflow3. sbatch