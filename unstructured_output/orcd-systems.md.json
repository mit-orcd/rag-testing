[
  {
    "element_id": "fc15afc97b2e45d1a45c566a5cba8c96",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "ORCD operates and provides support and training for a number of cluster computer systems available to all researchers. These systems all run with a Slurm scheduler and most have a web portal for interactive computing. These are Engaging, SuperCloud, Satori, and OpenMind.",
    "type": "NarrativeText"
  },
  {
    "element_id": "ace00bd7e092b9a47773a738b234199d",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "With the exception of SuperCloud, the maintenance schedule for all ORCD systems is:",
    "type": "NarrativeText"
  },
  {
    "element_id": "a44360982b4341efb4a39d53052d2ba4",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "SuperCloud has monthly downtimes on the 2nd Tuesday of each month.",
    "type": "NarrativeText"
  },
  {
    "element_id": "28e2dbd2e54ab886b713c7e9fe30b2d0",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "The Engaging cluster is a mixed CPU and GPU computing cluster that is openly available to all research projects at MIT. It has around 80,000 x86 CPU cores and 300 GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm resource scheduler that supports batch and interactive workloads and allows dedicated reservations. The cluster has a large shared file system for working datasets. Additional compute and storage resources can be purchased by PIs. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. A range of PI group maintained custom software stacks are also available through the widely adopted environment modules toolkit. A standard, open-source, web-based portal supporting Jupyter notebooks, R studio, Mathematica, and X graphics is available at https://engaging-ood.mit.edu. Further information and support is available from orcd-help-engaging@mit.edu.",
    "type": "NarrativeText"
  },
  {
    "element_id": "7b9155fe46c5e005639a95db2310f5cb",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "Accounts on the engaging cluster are connected to your main MIT institutional kerberos id. Connecting to the cluster for the first time through its web portal automatically activates an account with basic access to resources. See this page for instructions on how to log in.",
    "type": "NarrativeText"
  },
  {
    "element_id": "d65cc894fe395506231859c4b9fd2334",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "Satori is an IBM Power 9 large memory node system. It is open to everyone on campus and has optimized software stacks for machine learning and for image stack post-processing for MIT.nano Cryo-EM facilities. The system has 256 NVidia Volta GPU cards attached in groups of four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the Slurm resource scheduler that supports batch and interactive workloads and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. A standard web based portal https://satori-portal.mit.edu with Jupyter notebook support is available. Additional compute and storage resources can be purchased by PIs and integrated into the system. Further information and support is available at orcd-help-satori@mit.edu",
    "type": "NarrativeText"
  },
  {
    "element_id": "72e83a291ed21dd76a6a0f83578d2512",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "You can get an account by logging into https://satori-portal.mit.edu with your MIT credentials. This automatically activates an account with basic access to resources. See this page for more information.",
    "type": "NarrativeText"
  },
  {
    "element_id": "37d5ae72bd9bcd5876d495ca7ecb4243",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "The SuperCloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility is open to everyone on campus. The latest SuperCloud system has more than 16,000 x86 CPU cores and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource scheduler that supports batch and interactive workloads and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. A custom, web-based portal supporting Jupyter notebooks is available at https://txe1-portal.mit.edu/. Further information and support is available at supercloud@mit.edu.",
    "type": "NarrativeText"
  },
  {
    "element_id": "d899ea407347d7bd2155856f588fb200",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "To request a SuperCloud account follow the instructions on SuperCloud's Requesting an Account page.",
    "type": "NarrativeText"
  },
  {
    "element_id": "7be22161e5d12b2d41120375676c94a0",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "The OpenMind system is a collaboration with Department of Brain and Cognitive Sciences (BCS) and McGovern Institute. OpenMind is mainly a GPU computing cluster optimized for artificial intelligence (AI) research and data science. Totally there are around 70 compute nodes, 3500 CPU cores, 48 TB of RAM, and 340 GPUs, including 142 A100-80GB GPUs. It also provides around 2 PB of flash storage supporting fast read/write data speed. Hardware access is through the Slurm resource scheduler that supports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and Docker compatible Apptainer/Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. Further information and support is available at orcd-help-openmind@mit.edu.",
    "type": "NarrativeText"
  },
  {
    "element_id": "318bc2f24ae5d0e6f67ab03de0e04766",
    "metadata": {
      "filename": "orcd-systems.md",
      "filetype": "text/markdown",
      "languages": [
        "eng"
      ]
    },
    "text": "Accounts will be available for MIT users in 2024.",
    "type": "NarrativeText"
  }
]